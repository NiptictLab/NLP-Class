{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note of Measuring String Similarity\n",
    "## for spelling suggestion?!\n",
    "\n",
    "မြန်မာစာအတွက် လက်ရှိရှိနေတဲ့ string similarity measurement algorithm တွေက သုံးလို့တော့ရတယ်။  \n",
    "ဒါပေမဲ့ ငါသိနေတာက လက်ရှိရှိနေတဲ့ algorithm တွေက မြန်မာစာအတွက် perfect မဟုတ်ဘူး။\n",
    "\n",
    "ရှိပြီးသား နည်းတွေကို အခြေခံပြီးတော့ ငါတို့ မြန်မာစာ အတွက် algorithm အသစ်ရှာလို့ရနိုင်မလား။  \n",
    "အလွယ်ဆုံးကတော့ algorithm နှစ်မျိုးသုံးမျိုးကို ပေါင်းသုံးတာပဲ။ သို့သော် calculation time ...\n",
    "တစ်မျိုးထဲသုံးရင်တောင်မှ calculation time ကို ထည့်စဉ်းစားရတယ်။\n",
    "ဥပမာ စာမျက်နှာ ၁၀ မျက်နှာရှိတဲ့ မြန်မာစာကြောင်းတွေထဲက spelling mistake ရှာတဲ့ case\n",
    "\n",
    "Similarity or distance measurement က စာလုံးပေါင်းမှားသလား ဆိုတာကိုစစ်တဲ့ နေရာ (spelling detetion) မှာလည်း သုံးနိုင်သလို မှန်နိုင်ချေရှိတဲ့ စာလုံးတွေကို ဆွဲထုတ်တဲ့ နေရာ (spelling suggestion) မှာလည်း သုံးလို့ရတယ်။  \n",
    "\n",
    "String similarity algorithm တွေက အများကြီးရှိတယ်။  \n",
    "Algorithm တစ်ခုချင်း၊ တစ်ခုချင်း ရဲ့ အားသာချက်၊ အားနည်းချက်ကိုတော့ အနည်းဆုံး သိမှဖြစ်မယ်။  \n",
    "\n",
    "Go ahead for exploring some string similarity algorithms ... !   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Matching\n",
    "\n",
    "naive distance=0 ဆိုရင် စာကြောင်းနှစ်ကြောင်းက တူတယ်။\n",
    "Naive matching (or) exact matching စစ်ပုံစစ်နည်း တစ်ခုက"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def naive_matching(x, y):\n",
    "    found = None\n",
    "    if (len(x) == len(y)):\n",
    "        found = False\n",
    "        #print(range(len(y)-len(x)+1))\n",
    "        for i in range(len(y) - len(x) + 1): # looping over alignments\n",
    "            found = True\n",
    "            #print(range(len(x)))\n",
    "            for j in range(len(x)): # looping over characters\n",
    "                if y[i+j] != x[j]:\n",
    "                    print(\"y[i+j]: \", y[i+j], \" x[j]: \", x[j])\n",
    "                    found = False # မတူရင် alignment လုပ်စရာမလိုတော့ဘူး\n",
    "                    break\n",
    "    else:\n",
    "        print(\"Error: length of string1 ≠ length of string2!\")\n",
    "        \n",
    "    return found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_matching(\"ကခဂ\", \"ကခဂ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y[i+j]:  ဃ  x[j]:  ဂ\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_matching(\"ကခဂ\", \"ကခဃ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: length of string1 ≠ length of string2!\n"
     ]
    }
   ],
   "source": [
    "naive_matching(\"ကျောင်းသား\", \"ကျောင်းသူ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "အဲဒါကို Hamming Distance အတွက် ပြင်ရေးရင်"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def naive_Hamming_distance(x, y, maxDistance):\n",
    "    distance = None\n",
    "    if (len(x) == len(y)):\n",
    "        distance = 0\n",
    "        for i in range(len(y) - len(x) + 1):\n",
    "            found = True\n",
    "            for j in range(len(x)):\n",
    "                print(\"y[i+j]: \", y[i+j], \"x[j]:\", x[j])\n",
    "                if y[i+j] != x[j]:\n",
    "                    distance += 1\n",
    "            if distance > maxDistance:\n",
    "                break\n",
    "    else:\n",
    "        print(\"len(string1) ≠ len(string2)\")\n",
    "    return distance      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y[i+j]:  က x[j]: က\n",
      "y[i+j]:  ခ x[j]: ခ\n",
      "y[i+j]:  ဃ x[j]: ဂ\n",
      "y[i+j]:  ဂ x[j]: ဃ\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_Hamming_distance(\"ကခဂဃ\", \"ကခဃဂ\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(string1) ≠ len(string2)\n"
     ]
    }
   ],
   "source": [
    "naive_Hamming_distance(\"ကခဂဃင\", \"ကခဂ\", 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hamming Distance\n",
    "\n",
    "- two input string must be the same length\n",
    "- only one operation (i.e. substitution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hamming_distance(string1, string2): \n",
    "    if (len(string1) == len(string2)):\n",
    "        distance = 0\n",
    "        common_length = len(string1)\n",
    "        for i in range(common_length):\n",
    "            # if two char are different, +1\n",
    "            if string1[i] != string2[i]:\n",
    "                distance += 1\n",
    "        return distance\n",
    "    else:\n",
    "        print(\"Error: length of string1 ≠ length of string2!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hamming_distance(\"ကခဂ\", \"ကဂဂ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: length of string1 ≠ length of string2!\n"
     ]
    }
   ],
   "source": [
    "hamming_distance(\"မမဝဝထထက\", \"မမလှလှထထက\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hamming distance က word length တူတဲ့ကိစ္စမှာတော့ အသုံးဝင်တယ်"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Levenshtein Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "#InteractiveShell.ast_node_interactivity = \"all\"\n",
    "InteractiveShell.ast_node_interactivity = \"last_expr\"\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#ref: http://stackabuse.com/levenshtein-distance-and-text-similarity-in-python/\n",
    "\n",
    "def levenshtein(seq1, seq2):\n",
    "    size_x = len(seq1) + 1\n",
    "    size_y = len(seq2) + 1\n",
    "    matrix = np.zeros ((size_x, size_y))\n",
    "    for x in range(size_x):\n",
    "        matrix [x, 0] = x\n",
    "    for y in range(size_y):\n",
    "        matrix [0, y] = y\n",
    "\n",
    "    for x in range(1, size_x):\n",
    "        for y in range(1, size_y):\n",
    "            if seq1[x-1] == seq2[y-1]:\n",
    "                matrix [x,y] = min(\n",
    "                    matrix[x-1, y] + 1,\n",
    "                    matrix[x-1, y-1],\n",
    "                    matrix[x, y-1] + 1\n",
    "                )\n",
    "            else:\n",
    "                matrix [x,y] = min(\n",
    "                    matrix[x-1,y] + 1,\n",
    "                    matrix[x-1,y-1] + 1,\n",
    "                    matrix[x,y-1] + 1\n",
    "                )\n",
    "    print (matrix)\n",
    "    return (matrix[size_x - 1, size_y - 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9.]\n",
      " [ 1.  0.  1.  2.  3.  4.  5.  6.  7.  8.]\n",
      " [ 2.  1.  0.  1.  2.  3.  4.  5.  6.  7.]\n",
      " [ 3.  2.  1.  0.  1.  2.  3.  4.  5.  6.]\n",
      " [ 4.  3.  2.  1.  0.  1.  2.  3.  4.  5.]\n",
      " [ 5.  4.  3.  2.  1.  0.  1.  2.  3.  4.]\n",
      " [ 6.  5.  4.  3.  2.  1.  0.  1.  2.  3.]\n",
      " [ 7.  6.  5.  4.  3.  2.  1.  0.  1.  2.]\n",
      " [ 8.  7.  6.  5.  4.  3.  2.  1.  0.  1.]\n",
      " [ 9.  8.  7.  6.  5.  4.  3.  2.  1.  1.]]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(levenshtein(\"ရဲကျော်သူ\", \"ရဲကျော်သု\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  1.  2.  3.  4.]\n",
      " [ 1.  0.  1.  2.  3.]\n",
      " [ 2.  1.  0.  1.  2.]\n",
      " [ 3.  2.  1.  0.  1.]\n",
      " [ 4.  3.  2.  1.  1.]\n",
      " [ 5.  4.  3.  2.  1.]]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(levenshtein(\"ကခဂဃင\", \"ကခဂင\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  1.  2.  3.  4.  5.  6.  7.]\n",
      " [ 1.  0.  1.  2.  3.  4.  5.  6.]\n",
      " [ 2.  1.  0.  1.  2.  3.  4.  5.]\n",
      " [ 3.  2.  1.  0.  1.  2.  3.  4.]\n",
      " [ 4.  3.  2.  1.  0.  1.  2.  3.]\n",
      " [ 5.  4.  3.  2.  1.  1.  2.  3.]\n",
      " [ 6.  5.  4.  3.  2.  2.  1.  2.]\n",
      " [ 7.  6.  5.  4.  3.  3.  2.  1.]]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(levenshtein(\"ကိုကြီး\", \"ကိုကျီး\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  1.  2.  3.  4.  5.  6.  7.]\n",
      " [ 1.  1.  2.  3.  3.  4.  5.  6.]\n",
      " [ 2.  2.  1.  2.  3.  4.  5.  6.]\n",
      " [ 3.  3.  2.  1.  2.  3.  4.  5.]\n",
      " [ 4.  4.  3.  2.  1.  2.  3.  4.]\n",
      " [ 5.  5.  4.  3.  2.  1.  2.  3.]\n",
      " [ 6.  6.  5.  4.  3.  2.  1.  2.]\n",
      " [ 7.  7.  6.  5.  4.  3.  2.  1.]]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(levenshtein(\"ကိုကြီး\", \"ခိုကြီး\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  1.  2.  3.  4.  5.  6.  7.]\n",
      " [ 1.  0.  1.  2.  3.  4.  5.  6.]\n",
      " [ 2.  1.  0.  1.  2.  3.  4.  5.]\n",
      " [ 3.  2.  1.  0.  1.  2.  3.  4.]\n",
      " [ 4.  3.  2.  1.  1.  2.  3.  4.]\n",
      " [ 5.  4.  3.  2.  2.  2.  3.  4.]\n",
      " [ 6.  5.  4.  3.  3.  3.  2.  3.]\n",
      " [ 7.  6.  5.  4.  4.  4.  3.  2.]]\n",
      "2.0\n"
     ]
    }
   ],
   "source": [
    "print(levenshtein(\"ကိုကြီး\", \"ကိုဂျီး\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  1.  2.  3.  4.  5.  6.  7.]\n",
      " [ 1.  1.  2.  3.  3.  4.  5.  6.]\n",
      " [ 2.  2.  1.  2.  3.  4.  5.  6.]\n",
      " [ 3.  3.  2.  1.  2.  3.  4.  5.]\n",
      " [ 4.  4.  3.  2.  1.  2.  3.  4.]\n",
      " [ 5.  5.  4.  3.  2.  2.  3.  4.]\n",
      " [ 6.  6.  5.  4.  3.  3.  2.  3.]\n",
      " [ 7.  7.  6.  5.  4.  4.  3.  2.]]\n",
      "2.0\n"
     ]
    }
   ],
   "source": [
    "print(levenshtein(\"ကိုကြီး\", \"ခိုကျီး\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm from Michael Gilleland (https://goo.gl/aUYbqW)\n",
    "** To Do: calculate % of similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We have to understand Dynamic Programming\n",
    "\n",
    "- Definition of Insert!\n",
    "- Definition of Delete!\n",
    "- Definition of Substitute!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question: Why taking Minimum instead of Maximum?\n",
    "# Answer: Try taking Maximum instead of Minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Syllable အတွက် Levenshtein distance ကို တွက်ချင်ရင်\n",
    "## ဝင်လာတဲ့ စာကြောင်းနှစ်ကြောင်းကို Syllable ဖြတ်ပြီးတော့\n",
    "## စာကြောင်းနှစ်ခုရဲ့ Syllable တစ်လုံးချင်းစီကို တိုက်စစ်၊ တူရင် 1, မတူရင် 0 ဖြည့်\n",
    "## ပိုနေတဲ့ syllable (i.e for longer string) တွေကို 0 ဖြည့်ရင်\n",
    "## Syllable ကို ကိုယ်စားပြုတဲ့ string1, string2 ကို ရပြီ။\n",
    "## အဲဒီ string နှစ်ခုကို ပုံမှန်အတိုင်း Levenshtein distance တွက်ရင်\n",
    "## အဲဒီရလဒ်က syllable level Levenshtein distance ပဲ\n",
    "\n",
    "## ဒီ syllable level က စဉ်းစားသင့်တယ်။\n",
    "## I know character level only is not enough ...   &\n",
    "## syllable unit is important for Myanmar language\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Regular Expression Python Library ကို သုံးလို့ရအောင် import လုပ်တာ\n",
    "import re\n",
    "\n",
    "# စာလုံးတွေကို အုပ်စုဖွဲ့တာ (သို့) variable declaration လုပ်တာ\n",
    "# တကယ်လို့ syllable break လုပ်တဲ့ အခါမှာ မြန်မာစာလုံးချည်းပဲ သပ်သပ် လုပ်ချင်တာဆိုရင် enChar က မလိုပါဘူး\n",
    "myConsonant = \"က-အ\"\n",
    "enChar = \"a-zA-Z0-9\"\n",
    "otherChar = \"ဣဤဥဦဧဩဪဿ၌၍၏၀-၉၊။!-/:-@[-`{-~\\s\"\n",
    "ssSymbol = '္'\n",
    "ngaThat = 'င်'\n",
    "aThat = '်'\n",
    "\n",
    "# Regular expression pattern for Myanmar syllable breaking\n",
    "# *** a consonant not after a subscript symbol AND \n",
    "# a consonant is not followed by a-That character or a subscript symbol\n",
    "# မြန်မာစာကို syllable segmentation လုပ်ဖို့အတွက်က ဒီ RE pattern တစ်ခုတည်းနဲ့ အဆင်ပြေတယ်။\n",
    "BreakPattern = re.compile(r\"((?<!\" + ssSymbol + r\")[\"+ myConsonant + r\"](?![\" + aThat + ssSymbol + r\"])\" + r\"|[\" + enChar + otherChar + r\"])\", re.UNICODE)\n",
    "\n",
    "# sylbreak function ဆောက်တဲ့ အပိုင်း\n",
    "def sylbreak(line):\n",
    "       line = re.sub(r\"\\s+\",\"\", line)\n",
    "       line = BreakPattern.sub(r\" \" + r\"\\1\", line)\n",
    "       return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' မြန် မာ စာ သည် တို့ စာ ။ တို့ စာ ကို သု တေ သ န လုပ် ပါ ။'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sylbreak(\"မြန်မာစာသည် တို့စာ။ တို့စာကို သုတေသန လုပ်ပါ။\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: UTF-8 -*-\n",
    "def syl_check(line1, line2):\n",
    "    line1_syl = sylbreak(line1)\n",
    "    line2_syl = sylbreak(line2)\n",
    "    \n",
    "    line1_syl = line1_syl.strip()\n",
    "    line2_syl = line2_syl.strip()\n",
    "    \n",
    "    line1_array=line1_syl.split(\" \")\n",
    "    line2_array=line2_syl.split(\" \")\n",
    "    \n",
    "    #print(line1_array)\n",
    "    #print(line2_array)\n",
    "    max_length = max(len(line1_array), len(line2_array))\n",
    "    min_length = min(len(line1_array), len(line2_array))\n",
    "    #print(max_length)\n",
    "    #print(line1_syl)\n",
    "    #print(min_length)\n",
    "    #print(line2_syl)\n",
    "    \n",
    "    #twod=[[0 for j in range(len(line1_array))] for i in range(len(line2_array))]\n",
    "    #print(list(set(line1_array) & set(line2_array)))\n",
    "    \n",
    "    new_list1 = []\n",
    "    new_list2 = []\n",
    "        \n",
    "    for i in range(max_length):\n",
    "        #print(i)\n",
    "        if i < min_length:\n",
    "            if line1_array[i] == line2_array[i]:\n",
    "               new_list1.append(1)\n",
    "               new_list2.append(1)\n",
    "            else:\n",
    "                new_list1.append(0)\n",
    "                new_list2.append(1)\n",
    "        else:\n",
    "            new_list1.append(0)\n",
    "            new_list2.append(1)\n",
    "\n",
    "    #convertion list into string\n",
    "    checked_syl_str1 = ''.join(str(e) for e in new_list1)\n",
    "    checked_syl_str2 = ''.join(str(e) for e in new_list2)\n",
    "    \n",
    "    return checked_syl_str1, checked_syl_str2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('011', '111')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syl_check(\"လမင်းကြီး\", \"နေမင်းကြီး\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#str1, str2 = syl_check(\"လမင်းကြီး\", \"နေမင်းကြီး\")\n",
    "#str1, str2 = syl_check(\"လှလှပပ\", \"လှပ\")\n",
    "#str1, str2 = syl_check(\"လှပ\", \"လှလှပပ\")\n",
    "#str1, str2 = syl_check(\"ရှိတယ်\", \"မရှိ\")\n",
    "#str1, str2 = syl_check(\"ကခဂ\", \"ကခဃ\") \n",
    "#str1, str2 = syl_check(\"ကခဂဃင\", \"ကခဂဂဂ\")\n",
    "\n",
    "def char_syl_ld(string1, string2):\n",
    "    #print input strings\n",
    "    print(\"string1:\", string1)\n",
    "    print(\"string2:\", string2)\n",
    "    \n",
    "    ld=levenshtein(string1, string2)\n",
    "    print(\"Levenshtein distance (character unit): \", ld)\n",
    "    \n",
    "    syl_str1, syl_str2 = syl_check(string1, string2)\n",
    "    print(\"converted string:\", syl_str1)\n",
    "    print(\"converted string:\", syl_str2)\n",
    "    syl_ld = levenshtein(syl_str1, syl_str2)\n",
    "    print(\"Levenshtein distance (syllable unit):\", syl_ld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "string1: ဘာပြောတယ်\n",
      "string2: ဘပြေတယ်\n",
      "[[ 0.  1.  2.  3.  4.  5.  6.  7.]\n",
      " [ 1.  0.  1.  2.  3.  4.  5.  6.]\n",
      " [ 2.  1.  1.  2.  3.  4.  5.  6.]\n",
      " [ 3.  2.  1.  2.  3.  4.  5.  6.]\n",
      " [ 4.  3.  2.  1.  2.  3.  4.  5.]\n",
      " [ 5.  4.  3.  2.  1.  2.  3.  4.]\n",
      " [ 6.  5.  4.  3.  2.  2.  3.  4.]\n",
      " [ 7.  6.  5.  4.  3.  2.  3.  4.]\n",
      " [ 8.  7.  6.  5.  4.  3.  2.  3.]\n",
      " [ 9.  8.  7.  6.  5.  4.  3.  2.]]\n",
      "Levenshtein distance (character unit):  2.0\n",
      "converted string: 001\n",
      "converted string: 111\n",
      "[[ 0.  1.  2.  3.]\n",
      " [ 1.  1.  2.  3.]\n",
      " [ 2.  2.  2.  3.]\n",
      " [ 3.  2.  2.  2.]]\n",
      "Levenshtein distance (syllable unit): 2.0\n"
     ]
    }
   ],
   "source": [
    "char_syl_ld(\"ဘာပြောတယ်\", \"ဘပြေတယ်\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "string1: ဘာပြောတယ်\n",
      "string2: ဘာပြတယ်\n",
      "[[ 0.  1.  2.  3.  4.  5.  6.  7.]\n",
      " [ 1.  0.  1.  2.  3.  4.  5.  6.]\n",
      " [ 2.  1.  0.  1.  2.  3.  4.  5.]\n",
      " [ 3.  2.  1.  0.  1.  2.  3.  4.]\n",
      " [ 4.  3.  2.  1.  0.  1.  2.  3.]\n",
      " [ 5.  4.  3.  2.  1.  1.  2.  3.]\n",
      " [ 6.  5.  4.  3.  2.  2.  2.  3.]\n",
      " [ 7.  6.  5.  4.  3.  2.  3.  3.]\n",
      " [ 8.  7.  6.  5.  4.  3.  2.  3.]\n",
      " [ 9.  8.  7.  6.  5.  4.  3.  2.]]\n",
      "Levenshtein distance (character unit):  2.0\n",
      "converted string: 101\n",
      "converted string: 111\n",
      "[[ 0.  1.  2.  3.]\n",
      " [ 1.  0.  1.  2.]\n",
      " [ 2.  1.  1.  2.]\n",
      " [ 3.  2.  1.  1.]]\n",
      "Levenshtein distance (syllable unit): 1.0\n"
     ]
    }
   ],
   "source": [
    "char_syl_ld(\"ဘာပြောတယ်\", \"ဘာပြတယ်\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "string1: ဘာပြောတယ်\n",
      "string2: ဘာပတယ်\n",
      "[[ 0.  1.  2.  3.  4.  5.  6.]\n",
      " [ 1.  0.  1.  2.  3.  4.  5.]\n",
      " [ 2.  1.  0.  1.  2.  3.  4.]\n",
      " [ 3.  2.  1.  0.  1.  2.  3.]\n",
      " [ 4.  3.  2.  1.  1.  2.  3.]\n",
      " [ 5.  4.  3.  2.  2.  2.  3.]\n",
      " [ 6.  5.  4.  3.  3.  3.  3.]\n",
      " [ 7.  6.  5.  4.  3.  4.  4.]\n",
      " [ 8.  7.  6.  5.  4.  3.  4.]\n",
      " [ 9.  8.  7.  6.  5.  4.  3.]]\n",
      "Levenshtein distance (character unit):  3.0\n",
      "converted string: 101\n",
      "converted string: 111\n",
      "[[ 0.  1.  2.  3.]\n",
      " [ 1.  0.  1.  2.]\n",
      " [ 2.  1.  1.  2.]\n",
      " [ 3.  2.  1.  1.]]\n",
      "Levenshtein distance (syllable unit): 1.0\n"
     ]
    }
   ],
   "source": [
    "char_syl_ld(\"ဘာပြောတယ်\", \"ဘာပတယ်\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How about Cosine Similarity?\n",
    "\n",
    "Cosine similarity က အရမ်းအသုံးဝင်တာ ငါသိတယ်။  \n",
    "ဒါပေမဲ့ စာကြောင်းတိုတွေအတွက်၊ စာကြောင်း နှစ်ကြောင်းတည်းကိုပဲ cosine similarity တိုင်းလို့တော့ မဖြစ်နိုင်ဘူး။  \n",
    "Corpus level, Dictionary level သွားမှ ဖြစ်မယ်။  \n",
    "စာလုံးတွေကို တန်ဖိုးတစ်ခုပြောင်းတဲ့ ကိစ္စကို စဉ်းစားရမယ်လို့ ဆိုလိုတာ\n",
    "\n",
    "အလွယ်ဆုံးကတော့ character level measurement \n",
    "\n",
    "*** Spelling Checking မှာက တကယ်တမ်း ငါတို့ စဉ်းစားရမှာ က အနည်းဆုံး phrase level ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "def text2vectors(string12):\n",
    "    def vectorize(sentence, vocab):\n",
    "        return [sentence.split().count(i) for i in vocab]\n",
    "    vectorized_text = []\n",
    "    vocab = sorted(set(chain(*[i.lower().split() for i in string12])))\n",
    "    \n",
    "    for i in string12:\n",
    "        vectorized_text.append((i, vectorize(i, vocab)))\n",
    "    return vectorized_text, vocab\n",
    "\n",
    "def create_vectorizedText_vocab(string1, string2):\n",
    "\n",
    "    string12 = [string1, string2]\n",
    "    vectorized_text, vocab = text2vectors(string12)\n",
    "    return vectorized_text, vocab\n",
    "\n",
    "def cosine_similarity(u, v):\n",
    "    return np.dot(u,v) / (sqrt(np.dot(u,u)) * sqrt(np.dot(v,v)))\n",
    "\n",
    "def cos_sim(a, b):\n",
    "    \"\"\"Takes 2 vectors a, b and returns the cosine similarity according \n",
    "    to the definition of the dot product\"\"\"\n",
    "    dot_product = np.dot(a, b)\n",
    "    norm_a = np.linalg.norm(a)\n",
    "    norm_b = np.linalg.norm(b)\n",
    "    return dot_product / (norm_a * norm_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vt: [('နေကောင်းပါတယ်နော်။', [1, 0]), ('နေကောင်းပါတယ်ရှင်။', [0, 1])]\n",
      "နေကောင်းပါတယ်နော်။\n",
      "နေကောင်းပါတယ်နော်။\n",
      "Cosine similarity value = 1.0\n",
      "နေကောင်းပါတယ်နော်။\n",
      "နေကောင်းပါတယ်ရှင်။\n",
      "Cosine similarity value = 0.0\n",
      "နေကောင်းပါတယ်ရှင်။\n",
      "နေကောင်းပါတယ်နော်။\n",
      "Cosine similarity value = 0.0\n",
      "နေကောင်းပါတယ်ရှင်။\n",
      "နေကောင်းပါတယ်ရှင်။\n",
      "Cosine similarity value = 1.0\n",
      "=====\n",
      "vt: [('နေကောင်းပါတယ်နော်။', [1, 0]), ('သိပ်နေမကောင်းပါဘူးရှင့်။', [0, 1])]\n",
      "နေကောင်းပါတယ်နော်။\n",
      "နေကောင်းပါတယ်နော်။\n",
      "Cosine similarity value = 1.0\n",
      "နေကောင်းပါတယ်နော်။\n",
      "သိပ်နေမကောင်းပါဘူးရှင့်။\n",
      "Cosine similarity value = 0.0\n",
      "သိပ်နေမကောင်းပါဘူးရှင့်။\n",
      "နေကောင်းပါတယ်နော်။\n",
      "Cosine similarity value = 0.0\n",
      "သိပ်နေမကောင်းပါဘူးရှင့်။\n",
      "သိပ်နေမကောင်းပါဘူးရှင့်။\n",
      "Cosine similarity value = 1.0\n",
      "=====\n",
      "vt: [('နေကောင်းပါတယ်နော်။', [0, 1]), ('ကောင်းပါတယ်ရှင်။', [1, 0])]\n",
      "နေကောင်းပါတယ်နော်။\n",
      "နေကောင်းပါတယ်နော်။\n",
      "Cosine similarity value = 1.0\n",
      "နေကောင်းပါတယ်နော်။\n",
      "ကောင်းပါတယ်ရှင်။\n",
      "Cosine similarity value = 0.0\n",
      "ကောင်းပါတယ်ရှင်။\n",
      "နေကောင်းပါတယ်နော်။\n",
      "Cosine similarity value = 0.0\n",
      "ကောင်းပါတယ်ရှင်။\n",
      "ကောင်းပါတယ်ရှင်။\n",
      "Cosine similarity value = 1.0\n",
      "\n",
      " We have to make word segmentation ... \n",
      "\n",
      "vt: [('နေကောင်း ပါတယ် နော် ။', [1, 1, 1, 0, 1]), ('နေကောင်း ပါတယ် ရှင် ။', [1, 0, 1, 1, 1])]\n",
      "နေကောင်း ပါတယ် နော် ။\n",
      "နေကောင်း ပါတယ် နော် ။\n",
      "Cosine similarity value = 1.0\n",
      "နေကောင်း ပါတယ် နော် ။\n",
      "နေကောင်း ပါတယ် ရှင် ။\n",
      "Cosine similarity value = 0.75\n",
      "နေကောင်း ပါတယ် ရှင် ။\n",
      "နေကောင်း ပါတယ် နော် ။\n",
      "Cosine similarity value = 0.75\n",
      "နေကောင်း ပါတယ် ရှင် ။\n",
      "နေကောင်း ပါတယ် ရှင် ။\n",
      "Cosine similarity value = 1.0\n",
      "=====\n",
      "vt: [('နေကောင်း ပါတယ် နော် ။', [0, 1, 1, 1, 0, 0, 0, 0, 1]), ('သိပ် နေ မကောင်း ပါဘူး ရှင့် ။', [1, 0, 0, 0, 1, 1, 1, 1, 1])]\n",
      "နေကောင်း ပါတယ် နော် ။\n",
      "နေကောင်း ပါတယ် နော် ။\n",
      "Cosine similarity value = 1.0\n",
      "နေကောင်း ပါတယ် နော် ။\n",
      "သိပ် နေ မကောင်း ပါဘူး ရှင့် ။\n",
      "Cosine similarity value = 0.204124145232\n",
      "သိပ် နေ မကောင်း ပါဘူး ရှင့် ။\n",
      "နေကောင်း ပါတယ် နော် ။\n",
      "Cosine similarity value = 0.204124145232\n",
      "သိပ် နေ မကောင်း ပါဘူး ရှင့် ။\n",
      "သိပ် နေ မကောင်း ပါဘူး ရှင့် ။\n",
      "Cosine similarity value = 1.0\n",
      "=====\n",
      "vt: [('နေကောင်း ပါတယ် နော် ။', [0, 1, 1, 1, 0, 1]), ('ကောင်း ပါတယ် ရှင် ။', [1, 0, 0, 1, 1, 1])]\n",
      "နေကောင်း ပါတယ် နော် ။\n",
      "နေကောင်း ပါတယ် နော် ။\n",
      "Cosine similarity value = 1.0\n",
      "နေကောင်း ပါတယ် နော် ။\n",
      "ကောင်း ပါတယ် ရှင် ။\n",
      "Cosine similarity value = 0.5\n",
      "ကောင်း ပါတယ် ရှင် ။\n",
      "နေကောင်း ပါတယ် နော် ။\n",
      "Cosine similarity value = 0.5\n",
      "ကောင်း ပါတယ် ရှင် ။\n",
      "ကောင်း ပါတယ် ရှင် ။\n",
      "Cosine similarity value = 1.0\n",
      "=====\n",
      "vt: [('ကောင်းဘူး', [1, 0]), ('ဘာ', [0, 1])]\n",
      "ကောင်းဘူး\n",
      "ကောင်းဘူး\n",
      "Cosine similarity value = 1.0\n",
      "ကောင်းဘူး\n",
      "ဘာ\n",
      "Cosine similarity value = 0.0\n",
      "ဘာ\n",
      "ကောင်းဘူး\n",
      "Cosine similarity value = 0.0\n",
      "ဘာ\n",
      "ဘာ\n",
      "Cosine similarity value = 1.0\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain, product\n",
    "\n",
    "def calc_cosine_similarity(string1, string2):\n",
    "    # စာကြောင်းနှစ်ကြောင်းကို vector ဆောက်တယ်။ vocabulary list လည်းပြင်ဆင်တယ်။\n",
    "    vt, vocab = create_vectorizedText_vocab(string1, string2)\n",
    "    print(\"vt:\", vt)\n",
    "\n",
    "    for stringX, stringY in product(vt, vt):\n",
    "        print(stringX[0])\n",
    "        print(stringY[0])\n",
    "        print(\"Cosine similarity value =\", cosine_similarity(stringX[1], stringY[1]))\n",
    "        \n",
    "calc_cosine_similarity(\"နေကောင်းပါတယ်နော်။\", \"နေကောင်းပါတယ်ရှင်။\")\n",
    "print(\"=====\")\n",
    "calc_cosine_similarity(\"နေကောင်းပါတယ်နော်။\", \"သိပ်နေမကောင်းပါဘူးရှင့်။\")\n",
    "print(\"=====\")\n",
    "calc_cosine_similarity(\"နေကောင်းပါတယ်နော်။\", \"ကောင်းပါတယ်ရှင်။\")\n",
    "\n",
    "# We have to make word segmentation\n",
    "print(\"\\n We have to make word segmentation ... \\n\")\n",
    "calc_cosine_similarity(\"နေကောင်း ပါတယ် နော် ။\", \"နေကောင်း ပါတယ် ရှင် ။\")\n",
    "print(\"=====\")\n",
    "calc_cosine_similarity(\"နေကောင်း ပါတယ် နော် ။\", \"သိပ် နေ မကောင်း ပါဘူး ရှင့် ။\")\n",
    "print(\"=====\")\n",
    "calc_cosine_similarity(\"နေကောင်း ပါတယ် နော် ။\", \"ကောင်း ပါတယ် ရှင် ။\")\n",
    "print(\"=====\")\n",
    "calc_cosine_similarity(\"ကောင်းဘူး\", \"ဘာ\")\n",
    "\n",
    "#calc_cosine_similarity(\"နေကောင်း ပါတယ် နော် ။ အရင်က ထက်တော့\", \"နေကောင်းပါတယ်ရှင်။\")\n",
    "#calc_cosine_similarity(\"နေကောင်းပါတယ်နော်။\", \"နေကောင်းပါတယ်ရှင်။\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We have to consider consistency of word segmentation\n",
    "\n",
    "- Character level မဟုတ်ပဲ word level ဆိုရင်တော့ မြန်မာစာလုံးတွေကို အားလုံးညီနေအောင် segmentation လုပ်ထားမှ ဖြစ်မယ်။   \n",
    "မဟုတ်ရင် ပြဿနာရှိတယ်။  \n",
    "\n",
    "- Syllable level ဆိုရင်တော့ ငါတို့ consistency ကို control လုပ်လို့ရလိမ့်မယ်။ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine Similarity with Syllable Segmentation\n",
    "\n",
    "မြန်မာစာကို syllable segmentation ဖြတ်ပြီးမှ Cosine similarity တိုင်းကြည့်ရင်ကော  \n",
    "ရလဒ်က ဘယ်လိုနေမလဲ။  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " We have to make word segmentation ... \n",
      "\n",
      "vt: [('နေကောင်း ပါတယ် နော် ။', [1, 1, 1, 0, 1]), ('နေကောင်း ပါတယ် ရှင် ။', [1, 0, 1, 1, 1])]\n",
      "Cosine similarity value = 0.75\n",
      "=====\n",
      "vt: [('နေကောင်း ပါတယ် နော် ။', [0, 1, 1, 1, 0, 0, 0, 0, 1]), ('သိပ် နေ မကောင်း ပါဘူး ရှင့် ။', [1, 0, 0, 0, 1, 1, 1, 1, 1])]\n",
      "Cosine similarity value = 0.204124145232\n",
      "=====\n",
      "vt: [('နေကောင်း ပါတယ် နော် ။', [0, 1, 1, 1, 0, 1]), ('ကောင်း ပါတယ် ရှင် ။', [1, 0, 0, 1, 1, 1])]\n",
      "Cosine similarity value = 0.5\n",
      "\n",
      " How about syllable segmentation? \n",
      "\n",
      "vt: [(' နေ ကောင်း ပါ တယ် နော် ။', [1, 1, 1, 1, 1, 0, 1]), (' နေ ကောင်း ပါ တယ် ရှင် ။', [1, 1, 1, 0, 1, 1, 1])]\n",
      "Cosine similarity value = 0.833333333333\n",
      "=====\n",
      "vt: [(' နေ ကောင်း ပါ တယ် နော် ။', [1, 1, 1, 1, 1, 0, 0, 0, 0, 1]), (' သိပ် နေ မ ကောင်း ပါ ဘူး ရှင့် ။', [1, 0, 1, 0, 1, 1, 1, 1, 1, 1])]\n",
      "Cosine similarity value = 0.57735026919\n",
      "=====\n",
      "vt: [(' နေ ကောင်း ပါ တယ် နော် ။', [1, 1, 1, 1, 1, 0, 1]), (' ကောင်း ပါ တယ် ရှင် ။', [1, 1, 0, 0, 1, 1, 1])]\n",
      "Cosine similarity value = 0.73029674334\n"
     ]
    }
   ],
   "source": [
    "# Comparing between word level and syllable level cosine similarity\n",
    "\n",
    "def calc_cosine_similarity_laconic(string1, string2):\n",
    "    # စာကြောင်းနှစ်ကြောင်းကို vector ဆောက်တယ်။ vocabulary list လည်းပြင်ဆင်တယ်။\n",
    "    vt, vocab = create_vectorizedText_vocab(string1, string2)\n",
    "    print(\"vt:\", vt)\n",
    "    print(\"Cosine similarity value =\", cosine_similarity(vt[0][1], vt[1][1]))\n",
    "        \n",
    "print(\"\\n We have to make word segmentation ... \\n\")\n",
    "calc_cosine_similarity_laconic(\"နေကောင်း ပါတယ် နော် ။\", \"နေကောင်း ပါတယ် ရှင် ။\")\n",
    "print(\"=====\")\n",
    "calc_cosine_similarity_laconic(\"နေကောင်း ပါတယ် နော် ။\", \"သိပ် နေ မကောင်း ပါဘူး ရှင့် ။\")\n",
    "print(\"=====\")\n",
    "calc_cosine_similarity_laconic(\"နေကောင်း ပါတယ် နော် ။\", \"ကောင်း ပါတယ် ရှင် ။\")\n",
    "\n",
    "print(\"\\n How about syllable segmentation? \\n\")\n",
    "str1_syl = sylbreak(\"နေကောင်းပါတယ်နော်။\")\n",
    "str2_syl = sylbreak(\"နေကောင်းပါတယ်ရှင်။\")\n",
    "\n",
    "calc_cosine_similarity_laconic(str1_syl, str2_syl)\n",
    "print(\"=====\")\n",
    "\n",
    "str1_syl = sylbreak(\"နေကောင်းပါတယ်နော်။\")\n",
    "str2_syl = sylbreak(\"သိပ်နေမကောင်းပါဘူးရှင့်။\")\n",
    "calc_cosine_similarity_laconic(str1_syl, str2_syl)\n",
    "print(\"=====\")\n",
    "\n",
    "str1_syl = sylbreak(\"နေကောင်းပါတယ်နော်။\")\n",
    "str2_syl = sylbreak(\"ကောင်းပါတယ်ရှင်။\")\n",
    "calc_cosine_similarity_laconic(str1_syl, str2_syl)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing with Sklearn's cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vt: [(' နေ ကောင်း ပါ တယ် နာ် ။', [1, 1, 1, 1, 1, 0, 0, 0, 0, 1]), (' သိပ် နေ မ ကောင်း ပါ ဘူး ရှင့် ။', [1, 0, 0, 1, 1, 1, 1, 1, 1, 1])]\n",
      "Cosine similarity value = 0.57735026919\n",
      "vt: [(' နေ ကောင်း ပါ တယ် နာ် ။', [1, 1, 1, 1, 1, 0, 0, 0, 0, 1]), (' သိပ် နေ မ ကောင်း ပါ ဘူး ရှင့် ။', [1, 0, 0, 1, 1, 1, 1, 1, 1, 1])]\n",
      "0.5773502691896258\n",
      "=====\n",
      "vt: [(' နေ ကောင်း ပါ တယ် နော် ။', [1, 1, 1, 1, 1, 0, 1]), (' ကောင်း ပါ တယ် ရှင့် ။', [1, 1, 0, 0, 1, 1, 1])]\n",
      "Cosine similarity value = 0.73029674334\n",
      "vt: [(' နေ ကောင်း ပါ တယ် နော် ။', [1, 1, 1, 1, 1, 0, 1]), (' ကောင်း ပါ တယ် ရှင့် ။', [1, 1, 0, 0, 1, 1, 1])]\n",
      "0.5773502691896258\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics\n",
    "sklearn_cosine = sklearn.metrics.pairwise.cosine_similarity\n",
    "\n",
    "str1_syl = sylbreak(\"နေကောင်းပါတယ် နာ်။\")\n",
    "str2_syl = sylbreak(\"သိပ်နေမကောင်းပါဘူးရှင့်။\")\n",
    "calc_cosine_similarity_laconic(str1_syl, str2_syl)\n",
    "\n",
    "vt, vocab = create_vectorizedText_vocab(str1_syl, str2_syl)\n",
    "print(\"vt:\", vt)\n",
    " \n",
    "# for confirmation\n",
    "#print(\"binary_str1:\", vt[0][1])\n",
    "#print(\"binary_str1:\", vt[1][1])\n",
    "#print(\"type, vt[0][1]:\", type(vt[0][1]))\n",
    "\n",
    "#change into np.array\n",
    "bin_str1 = np.array([vt[0][1]])\n",
    "bin_str2 = np.array([vt[1][1]])\n",
    "\n",
    "#for confirmation\n",
    "#print(\"bin_str1:\", bin_str1)\n",
    "#print(\"bin_str2:\", bin_str2)\n",
    "#print(\"type:\", type(bin_str1))\n",
    "\n",
    "cosine_sim_score = sklearn_cosine(bin_str1, bin_str2)\n",
    "cosine_sim_score = cosine_sim_score.tolist()[0]\n",
    "print(cosine_sim_score[0])\n",
    "print(\"=====\")\n",
    "\n",
    "str1_syl = sylbreak(\"နေကောင်းပါတယ်နော်။\")\n",
    "str2_syl = sylbreak(\"ကောင်းပါတယ်ရှင့်။\")\n",
    "calc_cosine_similarity_laconic(str1_syl, str2_syl)\n",
    "\n",
    "vt, vocab = create_vectorizedText_vocab(str1_syl, str2_syl)\n",
    "print(\"vt:\", vt)\n",
    "cosine_sim_score = sklearn_cosine(bin_str1, bin_str2)\n",
    "cosine_sim_score = cosine_sim_score.tolist()[0]\n",
    "\n",
    "print(cosine_sim_score[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ngram Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ngrams(sentence, n):\n",
    "    return list(zip(*[sentence.split()[i:] for i in range(n)]))\n",
    "\n",
    "def test_ngrams(string1, string2):\n",
    "    # စာကြောင်းနှစ်ကြောင်းကို vector ဆောက်တယ်။ vocabulary list လည်းပြင်ဆင်တယ်။\n",
    "    vt, vocab = create_vectorizedText_vocab(string1, string2)\n",
    "    for string_i in vt:\n",
    "        print(string_i[0])\n",
    "        print(ngrams(string_i[0], 2))\n",
    "        print(ngrams(string_i[0], 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ngram list of word segmentation ... \n",
      "\n",
      "နေကောင်း ပါတယ် နော် ။\n",
      "[('နေကောင်း', 'ပါတယ်'), ('ပါတယ်', 'နော်'), ('နော်', '။')]\n",
      "[('နေကောင်း', 'ပါတယ်', 'နော်'), ('ပါတယ်', 'နော်', '။')]\n",
      "နေကောင်း ပါတယ် ရှင် ။\n",
      "[('နေကောင်း', 'ပါတယ်'), ('ပါတယ်', 'ရှင်'), ('ရှင်', '။')]\n",
      "[('နေကောင်း', 'ပါတယ်', 'ရှင်'), ('ပါတယ်', 'ရှင်', '။')]\n",
      "=====\n",
      "နေကောင်း ပါတယ် နော် ။\n",
      "[('နေကောင်း', 'ပါတယ်'), ('ပါတယ်', 'နော်'), ('နော်', '။')]\n",
      "[('နေကောင်း', 'ပါတယ်', 'နော်'), ('ပါတယ်', 'နော်', '။')]\n",
      "သိပ် နေ မကောင်း ပါဘူး ရှင့် ။\n",
      "[('သိပ်', 'နေ'), ('နေ', 'မကောင်း'), ('မကောင်း', 'ပါဘူး'), ('ပါဘူး', 'ရှင့်'), ('ရှင့်', '။')]\n",
      "[('သိပ်', 'နေ', 'မကောင်း'), ('နေ', 'မကောင်း', 'ပါဘူး'), ('မကောင်း', 'ပါဘူး', 'ရှင့်'), ('ပါဘူး', 'ရှင့်', '။')]\n",
      "=====\n",
      "နေကောင်း ပါတယ် နော် ။\n",
      "[('နေကောင်း', 'ပါတယ်'), ('ပါတယ်', 'နော်'), ('နော်', '။')]\n",
      "[('နေကောင်း', 'ပါတယ်', 'နော်'), ('ပါတယ်', 'နော်', '။')]\n",
      "ကောင်း ပါတယ် ရှင် ။\n",
      "[('ကောင်း', 'ပါတယ်'), ('ပါတယ်', 'ရှင်'), ('ရှင်', '။')]\n",
      "[('ကောင်း', 'ပါတယ်', 'ရှင်'), ('ပါတယ်', 'ရှင်', '။')]\n",
      "\n",
      " How about ngram list with syllable segmentation? \n",
      "\n",
      " နေ ကောင်း ပါ တယ် နော် ။\n",
      "[('နေ', 'ကောင်း'), ('ကောင်း', 'ပါ'), ('ပါ', 'တယ်'), ('တယ်', 'နော်'), ('နော်', '။')]\n",
      "[('နေ', 'ကောင်း', 'ပါ'), ('ကောင်း', 'ပါ', 'တယ်'), ('ပါ', 'တယ်', 'နော်'), ('တယ်', 'နော်', '။')]\n",
      " နေ ကောင်း ပါ တယ် ရှင် ။\n",
      "[('နေ', 'ကောင်း'), ('ကောင်း', 'ပါ'), ('ပါ', 'တယ်'), ('တယ်', 'ရှင်'), ('ရှင်', '။')]\n",
      "[('နေ', 'ကောင်း', 'ပါ'), ('ကောင်း', 'ပါ', 'တယ်'), ('ပါ', 'တယ်', 'ရှင်'), ('တယ်', 'ရှင်', '။')]\n",
      "=====\n",
      " နေ ကောင်း ပါ တယ် နော် ။\n",
      "[('နေ', 'ကောင်း'), ('ကောင်း', 'ပါ'), ('ပါ', 'တယ်'), ('တယ်', 'နော်'), ('နော်', '။')]\n",
      "[('နေ', 'ကောင်း', 'ပါ'), ('ကောင်း', 'ပါ', 'တယ်'), ('ပါ', 'တယ်', 'နော်'), ('တယ်', 'နော်', '။')]\n",
      " သိပ် နေ မ ကောင်း ပါ ဘူး ရှင့် ။\n",
      "[('သိပ်', 'နေ'), ('နေ', 'မ'), ('မ', 'ကောင်း'), ('ကောင်း', 'ပါ'), ('ပါ', 'ဘူး'), ('ဘူး', 'ရှင့်'), ('ရှင့်', '။')]\n",
      "[('သိပ်', 'နေ', 'မ'), ('နေ', 'မ', 'ကောင်း'), ('မ', 'ကောင်း', 'ပါ'), ('ကောင်း', 'ပါ', 'ဘူး'), ('ပါ', 'ဘူး', 'ရှင့်'), ('ဘူး', 'ရှင့်', '။')]\n",
      "=====\n",
      " နေ ကောင်း ပါ တယ် နော် ။\n",
      "[('နေ', 'ကောင်း'), ('ကောင်း', 'ပါ'), ('ပါ', 'တယ်'), ('တယ်', 'နော်'), ('နော်', '။')]\n",
      "[('နေ', 'ကောင်း', 'ပါ'), ('ကောင်း', 'ပါ', 'တယ်'), ('ပါ', 'တယ်', 'နော်'), ('တယ်', 'နော်', '။')]\n",
      " ကောင်း ပါ တယ် ရှင် ။\n",
      "[('ကောင်း', 'ပါ'), ('ပါ', 'တယ်'), ('တယ်', 'ရှင်'), ('ရှင်', '။')]\n",
      "[('ကောင်း', 'ပါ', 'တယ်'), ('ပါ', 'တယ်', 'ရှင်'), ('တယ်', 'ရှင်', '။')]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n ngram list of word segmentation ... \\n\")\n",
    "test_ngrams(\"နေကောင်း ပါတယ် နော် ။\", \"နေကောင်း ပါတယ် ရှင် ။\")\n",
    "\n",
    "print(\"=====\")\n",
    "test_ngrams(\"နေကောင်း ပါတယ် နော် ။\", \"သိပ် နေ မကောင်း ပါဘူး ရှင့် ။\")\n",
    "print(\"=====\")\n",
    "test_ngrams(\"နေကောင်း ပါတယ် နော် ။\", \"ကောင်း ပါတယ် ရှင် ။\")\n",
    "\n",
    "print(\"\\n How about ngram list with syllable segmentation? \\n\")\n",
    "str1_syl = sylbreak(\"နေကောင်းပါတယ်နော်။\")\n",
    "str2_syl = sylbreak(\"နေကောင်းပါတယ်ရှင်။\")\n",
    "\n",
    "test_ngrams(str1_syl, str2_syl)\n",
    "print(\"=====\")\n",
    "\n",
    "str1_syl = sylbreak(\"နေကောင်းပါတယ်နော်။\")\n",
    "str2_syl = sylbreak(\"သိပ်နေမကောင်းပါဘူးရှင့်။\")\n",
    "test_ngrams(str1_syl, str2_syl)\n",
    "print(\"=====\")\n",
    "\n",
    "str1_syl = sylbreak(\"နေကောင်းပါတယ်နော်။\")\n",
    "str2_syl = sylbreak(\"ကောင်းပါတယ်ရှင်။\")\n",
    "test_ngrams(str1_syl, str2_syl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "\n",
    "Once the unique n-grams for the term pair have been identified and counted, a quantitative similarity measure S between them can be computed in a number of ways, e.g., by using the Dice coefficient or the Overlap coefficient as follows.  \n",
    "\n",
    "Dice coefficient: S = 2C/(A+B)  \n",
    "\n",
    "Overlap coefficient: S = C/min(A,B)  \n",
    "\n",
    "where A is the number of unique n-grams in the first term, B the number of unique n-\n",
    "grams in the second term, and C the number of unique n-grams shared by both of them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To Do: Comparing Levenshtein Distance, Cosine Distance, Damerau-Levenshtein Distance, Jaro-Winkler Score and ngram Distance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Levenshtein Distance:\n",
      "[[  0.   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.]\n",
      " [  1.   0.   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.]\n",
      " [  2.   1.   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.]\n",
      " [  3.   2.   2.   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.]\n",
      " [  4.   3.   3.   2.   1.   2.   3.   4.   5.   6.   7.   8.   9.]\n",
      " [  5.   4.   4.   3.   2.   1.   2.   3.   4.   5.   6.   7.   8.]\n",
      " [  6.   5.   5.   4.   3.   2.   1.   2.   3.   4.   5.   6.   7.]\n",
      " [  7.   6.   6.   5.   4.   3.   2.   1.   2.   3.   4.   5.   6.]\n",
      " [  8.   7.   7.   6.   5.   4.   3.   2.   1.   2.   3.   4.   5.]\n",
      " [  9.   8.   8.   7.   6.   5.   4.   3.   2.   1.   2.   3.   4.]\n",
      " [ 10.   9.   9.   8.   7.   6.   5.   4.   3.   2.   1.   2.   3.]]\n",
      "3.0\n",
      "Cosine Similarity:\n",
      "vt: [(' ကျွန် တော် က', [1, 0, 1, 0, 1]), (' ကြွန် တော် ကို', [0, 1, 0, 1, 1])]\n",
      "Cosine similarity value = 0.333333333333\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'distance_unigrams_same' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-629bade96bad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mcalc_cosine_similarity_laconic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msylbreak\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msylbreak\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistance_unigrams_same\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"unigrams distance of str1, str2:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistance_bigrams_same\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'distance_unigrams_same' is not defined"
     ]
    }
   ],
   "source": [
    "#tr1_syl = sylbreak(\"နေကောင်းပါတယ်နော်။\")\n",
    "#str2_syl = sylbreak(\"ကောင်းပါတယ်ရှင်။\")\n",
    "str1 = \"ကျွန်တော်က\"\n",
    "str2 = \"ကြွန်တော်ကို\"\n",
    "\n",
    "print(\"Levenshtein Distance:\")\n",
    "print(levenshtein(str1, str2))\n",
    "\n",
    "print(\"Cosine Similarity:\")\n",
    "calc_cosine_similarity_laconic(sylbreak(str1), sylbreak(str2))\n",
    "\n",
    "dist = distance_unigrams_same(str1, str2)\n",
    "print(\"unigrams distance of str1, str2:\", dist)\n",
    "dist = distance_bigrams_same(str1, str2)\n",
    "print(\"bigrams distance of str1, str2:\", dist)\n",
    "dist = distance_trigrams_same(str1, str2)\n",
    "print(\"trigrams distance of str11, str22:\", dist)\n",
    "print(\"==========\")\n",
    "\n",
    "str1 = \"ကျွန် တော် က\"\n",
    "str2 = \"ကြွန် တော် ကို\"\n",
    "\n",
    "print(\"Levenshtein Distance:\")\n",
    "print(levenshtein(str1, str2))\n",
    "\n",
    "print(\"Cosine Similarity:\")\n",
    "calc_cosine_similarity_laconic(sylbreak(str1), sylbreak(str2))\n",
    "\n",
    "dist = distance_unigrams_same(str1, str2)\n",
    "print(\"unigrams distance of str1, str2:\", dist)\n",
    "dist = distance_bigrams_same(str1, str2)\n",
    "print(\"bigrams distance of str1, str2:\", dist)\n",
    "dist = distance_trigrams_same(str1, str2)\n",
    "print(\"trigrams distance of str11, str22:\", dist)\n",
    "\n",
    "print(\"==========\")\n",
    "\n",
    "str1 = \"ကျွန်တော်ကားကအနီရောင်\"\n",
    "str2 = \"ကြွန်တော်ကိုအနီရောင်ကြယ်ပေးပါ\"\n",
    "\n",
    "print(\"Levenshtein Distance:\")\n",
    "print(levenshtein(str1, str2))\n",
    "\n",
    "print(\"Cosine Similarity:\")\n",
    "calc_cosine_similarity_laconic(sylbreak(str1), sylbreak(str2))\n",
    "\n",
    "dist = distance_unigrams_same(str1, str2)\n",
    "print(\"unigrams distance of str1, str2:\", dist)\n",
    "dist = distance_bigrams_same(str1, str2)\n",
    "print(\"bigrams distance of str1, str2:\", dist)\n",
    "dist = distance_trigrams_same(str1, str2)\n",
    "print(\"trigrams distance of str1, str2:\", dist)\n",
    "print(\"==========\")\n",
    "\n",
    "str1 = \"ကျွန် တော် ကား က အ နီ ရောင်\"\n",
    "str2 = \"ကြွန် တော် ကို အ နီ ရောင် ကြယ် ပေး ပါ\"\n",
    "\n",
    "print(\"Levenshtein Distance:\")\n",
    "print(levenshtein(str1, str2))\n",
    "\n",
    "print(\"Cosine Similarity:\")\n",
    "calc_cosine_similarity_laconic(sylbreak(str1), sylbreak(str2))\n",
    "\n",
    "dist = distance_unigrams_same(str1, str2)\n",
    "print(\"unigrams distance of str1, str2:\", dist)\n",
    "dist = distance_bigrams_same(str1, str2)\n",
    "print(\"bigrams distance of str1, str2:\", dist)\n",
    "dist = distance_trigrams_same(str1, str2)\n",
    "print(\"trigrams distance of str1, str2:\", dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing with Korean (한글) Words\n",
    "\n",
    "I will used two Korean word pairs, (1) 국어 and 숙어 (2) 나무가지 and 나뭇가지.  \n",
    "\n",
    "Here,  \n",
    "   \"국어\" (national language)  \n",
    "   \"숙어\"(phrase)  \n",
    "   \n",
    "   \"나무가지\" (tree branch: old spelling standard)  \n",
    "   \"나뭇가지\" (tree branch: new spelling standard)  \n",
    "   \n",
    "For typing Korean characters, I used following online keyboard:  \n",
    "https://www.lexilogos.com/keyboard/korean.htm  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ko_str1 = \"국어\"\n",
    "ko_str2 = \"숙어\"\n",
    "\n",
    "print(\"ko_str1: \", ko_str1)\n",
    "print(\"ko_str2: \", ko_str2, \"\\n\")\n",
    "print(\"Levenshtein Distance:\")\n",
    "print(levenshtein(ko_str1, ko_str2))\n",
    "\n",
    "print(\"==========\")\n",
    "\n",
    "print(\"For ngram distance calculation:\")\n",
    "dist = distance_unigrams_same(ko_str1, ko_str2)\n",
    "print(\"unigrams distance of ko_str1, ko_str2:\", dist)\n",
    "print(\"\")\n",
    "\n",
    "ko_str1 = \"국 어\"\n",
    "ko_str2 = \"숙 어\"\n",
    "\n",
    "print(\"ko_str1: \", ko_str1)\n",
    "print(\"ko_str2: \", ko_str2, \"\\n\")\n",
    "\n",
    "\n",
    "dist = distance_bigrams_same(ko_str1, ko_str2)\n",
    "print(\"bigrams distance of ko_str1, ko_str2:\", dist)\n",
    "dist = distance_trigrams_same(ko_str1, ko_str2)\n",
    "print(\"trigrams distance of str1, str2:\", dist)\n",
    "print(\"==========\")\n",
    "\n",
    "print(\"\\n2nd Case:\")\n",
    "ko_str1 = \"나무가지\"\n",
    "ko_str2 = \"나뭇가지\"\n",
    "\n",
    "print(\"ko_str1: \", ko_str1)\n",
    "print(\"ko_str2: \", ko_str2, \"\\n\")\n",
    "print(\"Levenshtein Distance:\")\n",
    "print(levenshtein(ko_str1, ko_str2))\n",
    "print(\"==========\")\n",
    "print(\"For ngram distance calculation:\")\n",
    "\n",
    "dist = distance_unigrams_same(ko_str1, ko_str2)\n",
    "print(\"unigrams distance of ko_str1, ko_str2:\", dist)\n",
    "print(\"\")\n",
    "\n",
    "ko_str1 = \"나 무 가 지\"\n",
    "ko_str2 = \"나 뭇 가 지\"\n",
    "\n",
    "print(\"ko_str1: \", ko_str1)\n",
    "print(\"ko_str2: \", ko_str2, \"\\n\")\n",
    "\n",
    "dist = distance_bigrams_same(ko_str1, ko_str2)\n",
    "print(\"bigrams distance of ko_str1, ko_str2:\", dist)\n",
    "dist = distance_trigrams_same(ko_str1, ko_str2)\n",
    "print(\"trigrams distance of str1, str2:\", dist)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference Paper1: Edit Distance Calculation by Phonetic Rules and Word-length\n",
    "Normalization, Byunggul Bae, Seung-Shik Kang, Byung-Yeon Hwang  \n",
    "\n",
    "Reference Paper2: Word Similarity Calculation by Using the Edit Distance\n",
    "Metrics with Consonant Normalization, Seung-Shik Kang, 2015  \n",
    "\n",
    "အထက်ပါ စာတမ်းနှစ်စောင်ထဲမှာ ပြောပြထားသလို case နှစ်ခုစလုံးအတွက် edit distance က 1 ဖြစ်နေတယ်။   \n",
    "သူတို့ပေပါထဲမှာ ရေးထားတဲ့ 0.5 ဖြစ်သင့်တယ်၊ ထားချင်တယ်ဆိုတဲ့ အချက်ကို ငါရပြီ။  \n",
    "\n",
    "\n",
    "Proposal တစ်ခုက \"Edit Distance Normalization\"\n",
    "\n",
    "\n",
    "$$ N = \\frac{\\alpha}{ \\max(\\vert W_{1} \\vert, \\  \\vert W_{2} \\vert)} $$  \n",
    "\n",
    "Where, $$ \\vert W_{i} \\vert = 3 \\times syllable\\_length(W_{i}) $$\n",
    "\n",
    "\n",
    "for case 1:\n",
    "\n",
    "ko_str1:  국 어  \n",
    "ko_str2:  숙 어 \n",
    "\n",
    "$$ N = \\frac{1}{ \\max(\\vert 3 \\times 2 \\vert, \\  \\vert 3 \\times 2 \\vert)} $$  \n",
    "$$ N = \\frac{1}{ \\max(\\vert 6 \\vert, \\  \\vert 6 \\vert)} $$  \n",
    "$$ N = \\frac{1}{6} $$\n",
    "$$ N = 0.16 $$\n",
    "\n",
    "for case 2:\n",
    "\n",
    "ko_str1:  나무가지  \n",
    "ko_str2:  나뭇가지 \n",
    "\n",
    "$$ N = \\frac{1}{ \\max(\\vert 3 \\times 4 \\vert, \\  \\vert 3 \\times 4 \\vert)} $$  \n",
    "$$ N = \\frac{1}{12} $$\n",
    "$$ N = 0.08 $$  \n",
    "\n",
    "\n",
    "ဒီ ပရိုပိုဇယ်နဲ့ဆိုရင် အထက်ပါ case (1), case (2) မှာဆိုရင် ရှည်တဲ့စာကြောင်းက ပိုပြီးတော့ similarity ဖြစ်တယ်။  \n",
    "\n",
    "ဒီစာတမ်း ၂စောင် ကို အခြေခံပြီ: မြန်မာစာအတွက် detail study လုပ်သင့်တာက  \n",
    "\n",
    "1. Character based Edit Distance (FIN)\n",
    "2. Syllable based Edit Distance (FIN)\n",
    "3. Character and Syllable based Edit Distance, (i.e. Hybrid) (Not yet)\n",
    "4. Normalized Phoneme based Edit Distance (Not yet)\n",
    "5. Normalized Phoneme and Normalized Edit Distance (Not yet)\n",
    "\n",
    "----\n",
    "\n",
    "No. 3 က လုပ်လို့ရတယ်။ Error syllable ကို char level calculation လုပ်ပြီး၊ တခြား စာလုံးတွေက syllable level edit distance တွက်တာလို့ နားလည်တယ်။  \n",
    "No. 4 က မြန်မာစာအတွက် စဉ်းစားရမယ်။ သရအတွက် normalization လား။  \n",
    "No. 5 approach ကတော့ No. 4 ပြီးမှပဲ လုပ်လို့ရမယ်။    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#calculating average of list elememt values\n",
    "\n",
    "def average_calc(numbers):\n",
    "    try:\n",
    "        avg = sum(numbers)/len(numbers)\n",
    "        print(\"= \", sum(numbers), \"/\", len(numbers))  \n",
    "        return avg\n",
    "    except ZeroDivisionError:\n",
    "        return \"Zero Division Error!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average([1, 2, 3, 4, 5])\n",
    "average([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Damerau-Levenshtein Distance (DLD)\n",
    "\n",
    "DLD ကို တွက်ကြည့်ရန်\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Euclidean: Take the square root of the sum of the squares of the differences of the coordinates.\n",
    "\n",
    "For example, if x=(a,b) and y=(c,d), the Euclidean distance between x and y is\n",
    "\n",
    "(a−c)2+(b−d)2‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾√.\n",
    "\n",
    "Manhattan: Take the sum of the absolute values of the differences of the coordinates.\n",
    "\n",
    "For example, if x=(a,b) and y=(c,d), the Manhattan distance between x and y is\n",
    "\n",
    "|a−c|+|b−d|.\n",
    "\n",
    "For your vectors, it's the same thing except you have more coordinates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jaro-Winkler Score Calculation \n",
    "\n",
    "မြန်မာစာအတွက် Jaro-Winkler Score ကို တွက်ကြည့်ရင်ကော"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distance of Consonants + Distance of Vowels\n",
    "\n",
    "ဗျည်း အတွက် distance ကို သပ်သပ်တိုင်းပြီး  \n",
    "သရတွေ အတွက် distance ကို သပ်သပ် တိုင်းတာမျိုး ကော လုပ်လို့ရမလား။ အသုံးဝင်မလား။  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vowel Combination or Phoneme Similarity\n",
    "\n",
    "Word or String Similarity အတွက် အသံထွက်ကိုလည်း ထည့်သွင်းစဉ်းစားလို့ ရတယ်။  \n",
    "ဒါပေမဲ့ ငါလုပ်ခဲ့တဲ့ Grapheme to Phoneme experiment တွေကနေ ငါနားလည်ခဲ့တာက  \n",
    "ငါတို့ မြန်မာစာမှာက ရေးတော့အမှန် ဖတ်တော့အသံ ဆိုတဲ့ကိစ္စက ဖြေရှင်းရတာ မလွယ်ဘူး။\n",
    "\n",
    "အသံဆိုတာထက်၊ Vowel combination ကိုပဲ စဉ်းစားရင်ကော  \n",
    "ဆိုလိုတာက သရတွဲတွေကိုပဲ တိုက်စစ်ပြီး ဘယ်လောက်တူသလဲ similarity score ထုတ်တဲ့ကိစ္စ  \n",
    "*** ရေးတဲ့အသံကိုပဲ ယူစဉ်းစားတဲ့ ပုံစံ\n",
    "\n",
    "==========\n",
    "\n",
    "ျ၊ ြ\n",
    "ရ၊ ယ\n",
    "က၊ ဂ\n",
    "စ၊ ဆ\n",
    "လ၊ ဠ\n",
    "န၊ ဏ\n",
    "\n",
    "ငါ့ရဲ့ kKg keyboard mapping ပဲ\n",
    "\n",
    "==========\n",
    "\n",
    "Phonetic similarity paper တွေကို ပြန်ဖတ်ရန်။ ဒီကြားထဲမှာ စာတမ်းအသစ်တွေလည်း ရှိနိုင်တယ်။  \n",
    "\n",
    "==========\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity with Word2Vec\n",
    "\n",
    "ဒီနည်းကတော့ Corpus အပေါ်ကို မူတည်နေတယ်။  \n",
    "ကောင်းတာက meaning ဆိုတဲ့ concept က ပါဝင်တဲ့ ကိစ္စ။ သို့သော် လက်ရှိပြောနေကြတဲ့ meaning မှာက ပြဿနာတွေရှိသေးတယ်။  \n",
    "Meaning similarity ကို spelling checking အတွက် ထည့်စဉ်းစားပေးနိုင်ရင်တော့ intelligent ဖြစ်တယ်။  \n",
    "ငါသိရသလောက် လက်ရှိ string similarity တွေမှာက meaning ကို လုံးဝထည့်မစဉ်းစားထားကြဘူး။  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity with Syl2Vec for Myanmar Language\n",
    "\n",
    "ဒါကတော့ Myanmar specific approach ဖြစ်သွားလိမ့်မယ်။  \n",
    "တခုကောင်းတာက UCSY Corpus လောက်ရှိရင်ကို အတိုင်းအတာတစ်ခုအထိ Syllable similarity ကို တိုင်းတာလို့ ရပြီ။  \n",
    "ဒီ similarity က syllable ngram ထက် ဘယ်လောက်သာမှာလဲ။  \n",
    "Syllable meaning လား။  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Study on Myanmar Language Spelling Examples\n",
    "\n",
    "Followings are some spelling mistakes that I took from email, BBC and VOA news:\n",
    "\n",
    "- အဘိဓာန်အ<span style=\"color:red\">ပိုင်</span>ကိုကော update လုပ်ထားသေးလား။ \n",
    "\n",
    "- အဲဒီ ဒေသမှာ နေသူတွေကတော့ ကာကွယ်ရေး ဝန်ကြီးရဲ့ ပြောဆို မှု ကို လက်မခံပါဘူး။ အစိုးရ နဲ့ ကယ်ဆယ်ရေး အဖွဲ့တွေက <span style=\"color:red\">ပြစ်</span>ထား လို့ သူတို့ အနေနဲ့ ဒီလို ကြုံတွေ့ ရတာ ဆိုပြီး ဒေါသတကြီး ပြန်လည် တုန့်ပြန် ကြပါတယ်။ \n",
    "\n",
    "- တရုတ်က မြန်မာနိုင်ငံကို သုံးပြီးတော့ အိန္ဒိသမုဒ္ဒရာကို ကြီး<span style=\"color:red\">ဆိုး</span>လာနိုင်တယ်ဆိုတဲ့ မဟာဗျူဟာ ပြဿနာပေါ့။ ဒါကို Trump အစိုးရက ဂရုမစိုက်ဘူးဆိုတဲ့ သဘောလား။  \n",
    "\n",
    "- မိုးဒဏ် အပြင်းအထန်ခံစားခဲ့ရသူတွေကို ချက်ခြင်း ငွေကြေးထောက်ပံ့ဖို့နဲ့ ဆေးဝါး ကုသမှုခံယူဖို့ လိုအပ်နေသူ တွေကို အကူအညီတွေပေးရေး လမ်းညွှန်ချက်ကို လည်း <span style=\"color:red\">ထုန်</span>ပြန်လိုက်တယ်လို့လည်း ပြောခွင့်ရက ဆိုပါတယ်။ \n",
    "\n",
    "- ကျနော်တို့ဘဝမှာ တသက်လုံးကမကြားတဲ့ တရားရုံးအမိန့်တွေလည်း ကြားနေရတယ်၊ ဒါတွေက လုံးဝမဖြစ်သင့်ဘူး ။ အမှုလိုက်<span style=\"color:red\">ခဲ</span>တာ နှစ်ပေါင်းမနည်းဘူး၊ ဒီဟာမျိုး အမိန့်မျိုး တခါမှမကြားဖူးသေးဘူး။  \n",
    "\n",
    "- လတ်တ<span style=\"color:red\">လာ</span>ကာလအတွင်း နိုင်ငံတဝန်းဖြစ်ပွားနေတဲ့ ရေကြီး မြေပြိုမှုတွေကြောင့် ပြီးခဲ့တဲ့ ဇူလိုင်လ ၂၉ ရက်နေ့အထိ ၁၀ ဦး သေဆုံးခဲ့ပြီးဖြစ်တယ်လို့ ပြည်တွင်းသတင်းတွေမှာ ဖော်ပြနေပါတယ်။  \n",
    "\n",
    "- ဒီလို ဗိုလ်ချုပ်အောင်ဆန်း ကြေးရုပ်နဲ့ ပတ်သတ်လို့ ကန့်ကွက်တဲ့ ကယား ပြည်နယ် က ကရင်နီလူငယ်များ သမ<span style=\"color:red\">ဂ</span> အပါအဝင် အဖွဲ့တွေက လူငယ် ၂၃ယောက် ကို ပြည်နယ်အစိုးရက တရားစွဲဆိုထားပြီး လေးယောက်ကတော့ အခုလက်ရှိ လွိုင်ကော်အကျဉ်းထောင်ထဲမှာ ဖမ်းဆီးထားပါတယ်။  \n",
    "\n",
    "- “ဒေသတွင်း <span style=\"color:red\">နိ်ုင်</span>ငံတိုင်းက သူတို့ရေနံကို ရောင်းနေကြပြီး ကျနော်တို့က ဘာမှမလုပ်ဘဲ ထိုင်ကြည့်နေဖို့ဆိုတာ ဖြစ်နိုင်ပါသလား၊ သမိုင်းတလျှောက် ဒီရေလမ်းကြောင်း လုံခြုံရေးကို ကျနော်တို့ ထိန်းသိမ်းလာတာဆိုတာ မမေ့ပါနဲ့၊ ရေနံသယ်ပို့ လမ်းကြောင်း လုံခြုံရေးကို သမိုင်းနဲ့ချီပြီး ကျနော်တို့လုပ်လာတာပါ၊ ဒါကိုမမေ့ပါနဲ့” လို့ သမ္မတ Rouhani ကပြောပါတယ်။  \n",
    "\n",
    "- ဂျပန်နိုင်ငံမှာ စံချိန်တင်လောက်တဲ့ အပူလှိုင်းဖြတ်သန်းတာကြောင့် လူပေါင်း ၃၀ လောက်သေဆုံးပြီး ထောင်နဲ့ချီတဲ့ လူတွေလည်း အပူလှိုင်းဒဏ်ခံစားရတာကြောင့် နိုင်ငံတဝှမ်းက ဆေးရုံတွေ<span style=\"color:red\">စီ</span> တက်ရောက်ပြီး ကုသမှုခံယူနေပါတယ်။  \n",
    "\n",
    "- လိုဏ်ဂူထဲမှာ သီတင်းနှစ်ပတ်ကျော်ကြာ <span style=\"color:red\">ပိ်တ်</span>မိနေပြီးတဲ့နောက် ကယ်ထုတ်တဲ့အချိန်တုန်းက တယောက်ကို ကိုယ်အလေးချိ်န် ပျှမ်းမျှခြင်း ၉ ပေါင်စီလောက် ကျသွားကြပေမဲ့ ဆေးရုံတက် ကုသမှုခံယူပြီးတဲ့နောက် ၆ ပေါင်ခွဲစီ ပြန်တက်လာပြီလို့လည်း ဆရာဝန်တွေကပြောပါတယ်။  \n",
    "\n",
    "- မနက်ဖန်တနင်္ဂနွေနေ့မှာတော့ ရုရှားသမ္မတပူတင်နဲ့ တွေ့ဆုံ ဖို့ ဖင်လန်နိုင်ငံကို သွားရောက်မှာ ပါ။၃ ရက် ကြာ ခရီးစဉ်အဖြစ် ဗြိ<span style=\"color:red\">တိ်န်နိ်ုင်</span>ငံကို ရောက်နေတဲ့ သမ္မတ ထရမ့်ဟာ ဒီကနေ့မှာတော့ စကော့တလန်ကို ရောက်ရှိနေပြီး အဲဒီမှာ သူ့ကို ဆန့်ကျင်သူတွေက ကန့်ကွက်ဆန္ဒပြနေကြပါတယ်။  \n",
    "\n",
    "- အိုင်အက်<span style=\"color:red\">စ</span> အဖွဲ့ က အဲဒါသူတို့လက်ချက်လို့ ကြေညာထားပါတယ်။  \n",
    "\n",
    "- မစ္စတာ ရှာရစ်ဖ် ကို လာဟိုး လေဆိပ်ဆင်းလာကတည်း က အာဏာပိုင်တွေ က ဖမ်းခဲ့ပြီး မြို့တော် အစ္စလာမ်မာဘတ်<span style=\"color:red\">စ</span> ကို ခေါ်သွားမယ့် အထူးလေယာဉ်ပေါ်ကို ပြောင်းတင် ပေးခဲ့ပါတယ်။  \n",
    "\n",
    "- လှုပ်ရှားနိုင်တုန်းကို ဒေ<span style=\"color:red\">သာ</span>ရသလောက်အတူတူစုစေချင်တယ်။  \n",
    "\n",
    "- ဆွာဇီလန် ကနေ အက်ဆွာတီးနီ လို့ နာ<span style=\"color:red\">မယ်</span>ပြောင်း\n",
    "\n",
    "- GPU မှာ run လို့ရ<span style=\"color:red\">အော်</span> လုပ်မှ ဖြစ်လိမ့်မယ်။ \n",
    "\n",
    "- သမ္မတ ထရမ့်ကလည်း အီရန်ဟာ သမိုင်းမှာ နာ<span style=\"color:red\">မယ်</span>ဆိုး တွင်ရစ်လိမ့်မယ်လို့ တုံ့ ပြန် ခြိမ်းခြောက်ခဲ့တာ ဖြစ်ပါတယ်။\n",
    "\n",
    "- သာ<span style=\"color:red\">လီ</span>စွရခိုင်သင်္ကြန် (this is Rakhine language and not a spelling mistake) \n",
    "\n",
    "- မြန်မာနိုင်ငံမှာ မိုးတွင်းကာလမှာ တစ်နှစ်ကို ၃ ကြိမ်လောက်ရေကြီးတတ်တဲ့ အတွက် ကျန်တဲ့မိုးတွင်း ကာလတွေမှာ အခုလိုရေထပ်ကြီးမယ်ဆိုရင်တော့ ပျက်<span style=\"color:red\">ဆီး</span>မှု များလာနိုင်သလို ဆန်စပါးတင်ပို့မှုကလည်း ထိခိုက်လာနိုင်တယ်လို့ စိုက်ပျိုးရေးပညာရှင်အချို့က ပြောဆိုပါတယ်။\n",
    "\n",
    "- ၂၀၁၇ သြဂုတ်လကဖြစ်ပွားခဲ့တဲ့ ရခိုင်ပြည်နယ်မြောက်ပိုင်း ပဋိပက္ခကြောင့် ဘင်္ဂလားဒေ့ရှ်နိုင်ငံဘက် ထွက်ပြေးသွားကြတဲ့ ရိုဟင်ဂျာဒုက္ခသည် (၇)သိန်းတို့ရဲ့ပြဿနာ တနှစ်လောက် ရှိလာချိန်မှာ၊ ဒီပြ<span style=\"color:red\">သ</span>နာကို ရှေ့ဆက်ဘယ်လို ကိုယ်တွယ်ဖြေရှင်းသင့်သလဲ- ဆိုတဲ့ခေါင်းစဉ်နဲ့ အမေရိကန်ပြည်ထောင်စု ဝါရှင်တန်ဒီစီက Heritage Foundation မှာ ဇူလိုင်(၃၀)ရက် မနေ့တုန်းက ဆွေးနွေးပွဲ ကျင်းပခဲ့ပါတယ်။\n",
    "\n",
    "- နိုင်ငံတော်လှို့ဝှက်ချက်ဥပဒေနဲ့ တရားရုံးမှာ စွဲချက်တင် စစ်ဆေးခံနေရတဲ့ Reuter’s သတင်းထောက် ၂ ယောက်ထဲက ကိုကျော်စိုးဦးက သူ့ရဲ့လက်ကိုင်ဖုန်းထဲမှာ ရဲကတွေ့တယ်ဆိုတဲ့ အချက်အလက်တွေ ဘယ်လိုရောက်နေသလဲဆိုတာ သူမသိဘူးလို့ တရားရုံးမှာ တန<span style=\"color:red\">လာ</span>နေ့က ထွက်ဆိုခဲ့ပါတယ်။\n",
    "\n",
    "- တနင်္လာနေ့က ကြားနာစစ်ဆေးမှုအတွင်း သတင်းထောက်တွေဖမ်းဆီးခံရတဲ့ညက ကားမောင်းလိုက်ပို့ခဲ့<span style=\"color:red\"> တူ</span> Reuter’s သတင်းဌာနရဲ့ ယာဉ်မောင်း ကိုမျိုးသန့်ထွန်းကို သတင်းထောက်တွေရဲ့ရှေ့နေ့က သက်သေအဖြစ်ခေါ်ယူခဲ့ပါတယ်။\n",
    "\n",
    "- အခုဖွဲ့စည်းလိုက်တဲ့ လွတ်လပ်သော စုံစမ်းရေးကော်မရှင်မှာ ဥက္ကဌအဖြစ် ဖိလစ်ပိုင်နိုင်ငံ ဒုတိယ နိုင်ငံခြားရေးဝန်ကြီးဟောင်းလည်းဖြစ်၊ ကုလသမဂ္ဂ CEDAW ကော်မတီဥက္ကဌဟောင်းလည်းဖြစ်သူ သံအမတ်ကြီး ရိုဆာရီယိုမယ်နာလို Rosario Manalo ကဦးဆောင်ပြီး၊အဖွဲ့ဝင်တွေအဖြစ် ကုလသမဂ္ဂဆိုင်ရာ ဂျပန်အမြဲတမ်း ကိုယ်စားလှယ်ဟောင်းဖြစ်လည်းဖြစ်၊ ကုလသမဂ္ဂ လူသားချင်းစာနာမှု ကိစ္စရပ်များ ဆိုင်ရာ ဒုအတွင်းရေးမှုးချုပ်ဟောင်း သံအမတ်ကြီး ကန်ဇိုအိုရှီးမား Kenzo Oshima ၊ မြန်မာ<span style=\"color:red\">နိ်ုင်</span>ငံ နိုင်ငံတော်ဖွဲ့စည်းပုံ အခြေခံဥပဒေဆိုင်ရာ ခုံရုံးဥက္ကဌဟောင်း ဦးမြသိန်းနဲ့ UNICEF အဆင့်မြင့်အကြံပေးဟောင်းလည်းဖြစ် ကုလသမဂ္ဂဝန်ထမ်းကောလိပ် အဆင့်မြင့်အရာရှိ ဟောင်းလည်းဖြစ်တဲ့ ပါမောက္ခ ဒေါက်တာ အောင်ထွန်းသက်တို့လည်း ပါဝင်ပါတယ်။ \n",
    "\n",
    "- နူကလီးယားလက်နက်ဖျက်သိမ်းဖို့သဘောတူထားတဲ့မြောက်ကိုရီးယားရဲ့ ခြေလှမ်းတွေက ဘယ်လိုလဲ ဆိုတာနဲ့ပတ်သက်ပြီး ကိုကျော်အောင်လွင်က တင်ပြပေးထားပါတယ်။ (နြူကလီးယား၊ နူကလီးယားလား ... foreign word ကိစ္စ)\n",
    "\n",
    "- လူတွေရဲ့ ကျန်းမာရေး အတွက် ပြဿနာတွေရှိလာမှာ၊ ဝမ်းရောဂါတွေဖြစ်လာကြောက်လို့ စခန်းတိုင်းမှာ ကျန်းမာရေးအဖွဲ့တွေထားပေးထားတယ်။ (how to detect this kind of error, let it be ...)\n",
    "\n",
    "အထက်ပါ အမှားတွေကို အကြမ်းမျဉ်း အုပ်စုခွဲကြည့်ရင် အောက်ပါအတိုင်း    \n",
    "\n",
    "- Errors because of similar pronunciation\n",
    "- Typing mistake \n",
    "- Language Knowledge \n",
    "- Encoding (for us Zawgyi and Unicode font conversion) \n",
    "- Foreign word (this is difficult to make a decision) \n",
    "\n",
    "I have to find out how many types of spelling mistakes in Myanmar language !!!\n",
    "I should know percentage (%) of each spelling mistake ... (အချိန်ပေးရလိမ့်မယ်) \n",
    "*** English context also consider \n",
    "\n",
    "I should understand well on this ... that will help for both spelling error detection and suggestion  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
